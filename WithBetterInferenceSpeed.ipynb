{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "s7cG1eJDYF7Y",
    "outputId": "b64a6007-96e8-4151-e936-a3620f4d33ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "depthwise_conv2d_8 (Depthwis (None, 26, 26, 1)         10        \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 20, 20, 64)        3200      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 128)               819328    \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 823,828\n",
      "Trainable params: 823,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.2637 - acc: 0.9179 - val_loss: 0.0558 - val_acc: 0.9817\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.0880 - acc: 0.9736 - val_loss: 0.0430 - val_acc: 0.9857\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.0662 - acc: 0.9805 - val_loss: 0.0357 - val_acc: 0.9875\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.0569 - acc: 0.9831 - val_loss: 0.0305 - val_acc: 0.9902\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.0500 - acc: 0.9849 - val_loss: 0.0295 - val_acc: 0.9901\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.0442 - acc: 0.9861 - val_loss: 0.0264 - val_acc: 0.9907\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.0392 - acc: 0.9878 - val_loss: 0.0278 - val_acc: 0.9915\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0341 - acc: 0.9898 - val_loss: 0.0274 - val_acc: 0.9910\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.0323 - acc: 0.9897 - val_loss: 0.0246 - val_acc: 0.9929\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0293 - acc: 0.9909 - val_loss: 0.0246 - val_acc: 0.9920\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0276 - acc: 0.9912 - val_loss: 0.0270 - val_acc: 0.9907\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0255 - acc: 0.9921 - val_loss: 0.0252 - val_acc: 0.9921\n",
      "Test loss: 0.025193355914763925\n",
      "Test accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, DepthwiseConv2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(DepthwiseConv2D( kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (7, 7), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, to reduce the inference time I have taken below steps:\n",
    "--------------------------------------------------------------------------------------- \n",
    "I have compared both the codes on google colab and below are the comparisons and explanation for the changes, I have made to meet the requirements.\n",
    "\n",
    "Comparison of the Original Code and Updated Code:\n",
    "\n",
    "**Original Code (Which I got from Sudesh):**\n",
    "\n",
    "Total params: 1,199,882\n",
    "\n",
    "Trainable params: 1,199,882\n",
    "\n",
    "Non-trainable params: 0\n",
    "\n",
    "Test loss: 0.035234447981064565\n",
    "\n",
    "Test accuracy: 0.991\n",
    "\n",
    "\n",
    "**Updated Code:**\n",
    "\n",
    "Total params: 823,828\n",
    "\n",
    "Trainable params: 823,828\n",
    "\n",
    "Non-trainable params: 0\n",
    "\n",
    "Test loss: 0.025193355914763925\n",
    "\n",
    "Test accuracy: 0.9921\n",
    "\n",
    "\n",
    "\n",
    "1. I have replaced the first Conv2D layer with DepthwiseConv2D layer (with 3X3 kernel). \n",
    "\n",
    "=> **Reason:** These type of CNN’s (i.e., DepthWiseConv2D) are widely used because of the following two reasons –\n",
    "\n",
    "They have lesser number of parameters to adjust as compared to the standard CNN’s, which reduces overfitting.\n",
    "\n",
    "They are computationally cheaper because of fewer computations which makes them work faster.\n",
    "\n",
    "\n",
    "2. In the second layer of Conv2D, I have changed the kernel size to (7X7).\n",
    "\n",
    "=> **Reason:** A larger size kernel can overlook at the features and could skip the essential details in the images whereas a smaller size kernel could provide more information leading to more confusion. But here, I have tested the model with 5X5 and 7X7 both and the model was performing slightly worse than the model with 5X5 kernel with a huge difference in the number of parameters. So, I decided to keep 7X7 kernel.\n",
    "\n",
    "These changes, have made the training faster. Though, there are many changes need to be implemented to make the code tun more efficiently but this is the start in the same direction."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WithBetterInferenceSpeed.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
